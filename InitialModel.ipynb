{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n1IQY3Ucb5Qq"
      },
      "outputs": [],
      "source": [
        "pip install cohere hnswlib unstructured -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml6S8x9klpXu",
        "outputId": "a0cc5507-8e16-4e67-9201-5080707c53fc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
      ],
      "metadata": {
        "id": "vWgmUi_7rXbW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade --quiet  docx2txt"
      ],
      "metadata": {
        "id": "QP97sr1FrGyQ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-pptx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa_qh6UX4TH3",
        "outputId": "cf7a8c16-eac3-4563-e826-723c331739f0"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-pptx\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/471.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/471.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.0 python-pptx-0.6.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "import os\n",
        "import hnswlib\n",
        "import json\n",
        "import uuid\n",
        "from typing import List, Dict\n",
        "from unstructured.partition.html import partition_html\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# everything to under\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_community.document_loaders import Docx2txtLoader\n",
        "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
        "\n",
        "co = cohere.Client(userdata.get(\"COHERE_API_KEY\"))"
      ],
      "metadata": {
        "id": "ITWYUZ71d4na"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example local sources\n",
        "sources = [\n",
        "    {\n",
        "        \"title\": \"Capstone Project Managers\",\n",
        "        \"path\": \"/content/local_data/CAPSTONEPM.pdf\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"YWCC\",\n",
        "        \"path\": \"/content/local_data/YWCC.pdf\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Schedule\",\n",
        "        \"path\": \"/content/local_data/LIVESCHEDULE.csv\"\n",
        "    },\n",
        "     {\n",
        "        \"title\": \"Scope\",\n",
        "        \"path\":\"/content/local_data/ScopeDocument.docx\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"CISCO Track\",\n",
        "        \"path\": \"/content/local_data/CISCO.pptx\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "D3vza9mxj4Wm"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#document class\n",
        "class Documents:\n",
        "\n",
        "    def __init__(self, sources: List[Dict[str, str]]):\n",
        "        self.sources = sources # all sources not chunked\n",
        "        self.docs = [] # chunked sources\n",
        "        self.docs_embs = [] # embedded + chunked\n",
        "        self.retrieve_top_k = 10\n",
        "        self.rerank_top_k = 3\n",
        "        self.load()\n",
        "        self.embed()\n",
        "        self.index()\n",
        "\n",
        "    def loadGeneral(self,source):\n",
        "      if source[\"path\"].endswith(\".pdf\"):\n",
        "        loader = PyPDFLoader(source['path'])\n",
        "      elif source[\"path\"].endswith(\".docx\"):\n",
        "        loader = Docx2txtLoader(source[\"path\"])\n",
        "      elif source[\"path\"].endswith(\".pptx\"):\n",
        "        loader = UnstructuredPowerPointLoader(source[\"path\"])\n",
        "\n",
        "      docs = loader.load()\n",
        "      text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, add_start_index=True)\n",
        "      all_splits = text_splitter.split_documents(docs)\n",
        "      for split in all_splits:\n",
        "        self.docs.append(\n",
        "            {\n",
        "                \"title\": source[\"title\"],\n",
        "                \"text\": split.page_content,\n",
        "                \"path\": source[\"path\"]\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def loadCSV(self, source):\n",
        "      loader = CSVLoader(\n",
        "      file_path=source[\"path\"],\n",
        "      csv_args={\n",
        "          \"delimiter\": \",\",\n",
        "          \"quotechar\": '\"',\n",
        "        },\n",
        "      )\n",
        "\n",
        "      data = loader.load()\n",
        "      for doc in data:\n",
        "        self.docs.append(\n",
        "            {\n",
        "                \"title\": source[\"title\"],\n",
        "                \"text\": doc.page_content,\n",
        "                \"path\": source[\"path\"]\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def load(self):\n",
        "      print(\"loading docs\")\n",
        "      for source in self.sources:\n",
        "        if source[\"path\"].endswith(\".pdf\") or source[\"path\"].endswith(\".docx\") or source[\"path\"].endswith(\".pptx\"):\n",
        "          self.loadGeneral(source)\n",
        "        else:\n",
        "          self.loadCSV(source)\n",
        "\n",
        "    def embed(self):\n",
        "      print(\"embedding docs\")\n",
        "      batch_size = 90\n",
        "      self.docs_len = len(self.docs)\n",
        "      print(self.docs_len)\n",
        "      for i in range(0, self.docs_len, batch_size):\n",
        "        batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
        "        texts = [item[\"text\"] for item in batch]\n",
        "        docs_embs_batch = co.embed(\n",
        "            texts = texts,\n",
        "            model = \"embed-english-v3.0\",\n",
        "            input_type = \"search_document\"\n",
        "        ).embeddings\n",
        "        self.docs_embs.extend(docs_embs_batch)\n",
        "\n",
        "    def index(self):\n",
        "      print(\"Indexing documents...\")\n",
        "\n",
        "      self.index = hnswlib.Index(space=\"ip\", dim=1024)\n",
        "      self.index.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
        "      self.index.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
        "\n",
        "      print(f\"Indexing complete with {self.index.get_current_count()} documents.\")\n",
        "\n",
        "    def retrieve(self, query: str) -> List[Dict[str,str]]:\n",
        "      \"\"\"\n",
        "      Retrieves documents based on the given query.\n",
        "\n",
        "      Parameters:\n",
        "      query (str): The query to retrieve documents for.\n",
        "\n",
        "      Returns:\n",
        "      List[Dict[str, str]]: A list of dictionaries representing the retrieved  documents, with 'title', 'snippet', and 'url' keys.\n",
        "      \"\"\"\n",
        "      docs_retrieved = []\n",
        "      query_emb = co.embed(\n",
        "                  texts=[query],\n",
        "                  model=\"embed-english-v3.0\",\n",
        "                  input_type=\"search_query\"\n",
        "                  ).embeddings\n",
        "\n",
        "      doc_ids = self.index.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
        "      docs_to_rerank = []\n",
        "      for doc_id in doc_ids:\n",
        "          docs_to_rerank.append(self.docs[doc_id][\"text\"])\n",
        "\n",
        "      rerank_results = co.rerank(\n",
        "          query=query,\n",
        "          documents=docs_to_rerank,\n",
        "          top_n=self.rerank_top_k,\n",
        "          model=\"rerank-english-v2.0\",\n",
        "      )\n",
        "\n",
        "      doc_ids_reranked = []\n",
        "      for result in rerank_results:\n",
        "          doc_ids_reranked.append(doc_ids[result.index])\n",
        "\n",
        "      for doc_id in doc_ids_reranked:\n",
        "          docs_retrieved.append(\n",
        "              {\n",
        "                  \"title\": self.docs[doc_id][\"title\"],\n",
        "                  \"text\": self.docs[doc_id][\"text\"],\n",
        "                  \"path\": self.docs[doc_id][\"path\"],\n",
        "              }\n",
        "          )\n",
        "\n",
        "      return docs_retrieved\n"
      ],
      "metadata": {
        "id": "HZPjktb3iexp"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = Documents(sources)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "616IFmHX1SAz",
        "outputId": "14267de8-95fe-4026-b46c-992f8d1281a8"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dc\n",
            "read a csv Schedule\n",
            "embedding docs\n",
            "92\n",
            "Indexing documents...\n",
            "Indexing complete with 92 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Chatbot:\n",
        "  def __init__(self, docs: Documents):\n",
        "    self.docs = docs\n",
        "    self.conversation_id = str(uuid.uuid4())\n",
        "\n",
        "  def generate_response(self, message: str):\n",
        "    \"\"\"\n",
        "    Generates a response to the user's message.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): The user's message.\n",
        "\n",
        "    Yields:\n",
        "    Event: A response event generated by the chatbot.\n",
        "\n",
        "    Returns:\n",
        "    List[Dict[str, str]]: A list of dictionaries representing the retrieved documents.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate search queries (if any)\n",
        "    response = co.chat(message=message, search_queries_only=True)\n",
        "    # If there are search queries, retrieve documents and respond\n",
        "    if response.search_queries:\n",
        "        print(\"Retrieving information...\")\n",
        "\n",
        "        documents = self.retrieve_docs(response)\n",
        "\n",
        "        response = co.chat(\n",
        "            message=message,\n",
        "            documents=documents,\n",
        "            conversation_id=self.conversation_id,\n",
        "            stream=True,\n",
        "        )\n",
        "        for event in response:\n",
        "            yield event\n",
        "        yield response\n",
        "\n",
        "    # If there is no search query, directly respond\n",
        "    else:\n",
        "        response = co.chat(\n",
        "            message=message,\n",
        "            conversation_id=self.conversation_id,\n",
        "            stream=True\n",
        "        )\n",
        "        for event in response:\n",
        "            yield event\n",
        "\n",
        "\n",
        "\n",
        "  def retrieve_docs(self, response) -> List[Dict[str, str]]:\n",
        "      \"\"\"\n",
        "      Retrieves documents based on the search queries in the response.\n",
        "\n",
        "      Parameters:\n",
        "      response: The response object containing search queries.\n",
        "\n",
        "      Returns:\n",
        "      List[Dict[str, str]]: A list of dictionaries representing the retrieved documents.\n",
        "\n",
        "      \"\"\"\n",
        "      # Get the query(s)\n",
        "      queries = []\n",
        "      for search_query in response.search_queries:\n",
        "          queries.append(search_query[\"text\"])\n",
        "\n",
        "      # Retrieve documents for each query\n",
        "      retrieved_docs = []\n",
        "      for query in queries:\n",
        "          retrieved_docs.extend(self.docs.retrieve(query))\n",
        "\n",
        "      return retrieved_docs\n",
        "\n"
      ],
      "metadata": {
        "id": "sNNwf5Wiy5vy"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class App:\n",
        "    def __init__(self, chatbot: Chatbot):\n",
        "        \"\"\"\n",
        "        Initializes an instance of the App class.\n",
        "\n",
        "        Parameters:\n",
        "        chatbot (Chatbot): An instance of the Chatbot class.\n",
        "\n",
        "        \"\"\"\n",
        "        self.chatbot = chatbot\n",
        "\n",
        "    def run(self):\n",
        "      \"\"\"\n",
        "      Runs the chatbot application.\n",
        "      \"\"\"\n",
        "      while True:\n",
        "          # Get the user message\n",
        "          message = input(\"User: \")\n",
        "\n",
        "          # Typing \"quit\" ends the conversation\n",
        "          if message.lower() == \"quit\":\n",
        "              print(\"Ending chat.\")\n",
        "              break\n",
        "          else:\n",
        "              print(f\"User: {message}\")\n",
        "              # Get the chatbot response\n",
        "              response = self.chatbot.generate_response(message)\n",
        "\n",
        "              # Print the chatbot response\n",
        "              print(\"Chatbot:\")\n",
        "              citations_flag = False\n",
        "              docnames = {}\n",
        "              for event in response:\n",
        "                  stream_type = type(event).__name__\n",
        "\n",
        "                  # Text\n",
        "                  if stream_type == \"StreamTextGeneration\":\n",
        "                      print(event.text, end=\"\")\n",
        "\n",
        "                  # Citations\n",
        "                  if stream_type == \"StreamCitationGeneration\":\n",
        "                      if not citations_flag:\n",
        "                          print(\"\\n\\nCITATIONS:\")\n",
        "                          citations_flag = True\n",
        "                      print(event.citations[0])\n",
        "\n",
        "                  # Documents\n",
        "                  if citations_flag:\n",
        "                      if stream_type == \"StreamingChat\":\n",
        "                          print(\"\\n\\nDOCUMENTS:\")\n",
        "                          documents = [{'id': doc['id'],\n",
        "                                      'text': doc['text'][:50] + '...',\n",
        "                                      'title': doc['title'],\n",
        "                                      'path': doc['path']}\n",
        "                                      for doc in event.documents]\n",
        "                          for doc in documents:\n",
        "                            if doc[\"title\"] in docnames:\n",
        "                              continue\n",
        "                            else:\n",
        "                              docnames[doc[\"title\"]] = doc[\"path\"]\n",
        "              for docname in docnames:\n",
        "                print(docname + \": \" + docnames[docname])\n",
        "              print(f\"\\n{'-'*100}\\n\")\n"
      ],
      "metadata": {
        "id": "rxR0siP02qDN"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = Documents(sources)\n",
        "chatbot = Chatbot(documents)\n",
        "\n",
        "app = App(chatbot)\n",
        "\n",
        "app.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnHxy-ml29Vd",
        "outputId": "81497813-2c13-4467-b091-41d7c47e1797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dc\n",
            "read a csv Schedule\n",
            "embedding docs\n",
            "94\n",
            "Indexing documents...\n",
            "Indexing complete with 94 documents.\n",
            "User: when is the final presentation due? what is the final grade breakdown\n",
            "User: when is the final presentation due? what is the final grade breakdown\n",
            "Chatbot:\n",
            "Retrieving information...\n",
            "The final presentation is due on April 26, 2024. This presentation will be worth 10% towards your final grade for the semester. The final grade breakdown is as follows:\n",
            "\n",
            "- Presentation: 10%\n",
            "- Prototype demonstrations: 40%\n",
            "- Written report: 10%\n",
            "- Individual project: 15%\n",
            "- Team project: 15%\n",
            "\n",
            "CITATIONS:\n",
            "{'start': 33, 'end': 48, 'text': 'April 26, 2024.', 'document_ids': ['doc_0']}\n",
            "{'start': 81, 'end': 84, 'text': '10%', 'document_ids': ['doc_0']}\n",
            "\n",
            "\n",
            "DOCUMENTS:\n",
            "Schedule: /content/local_data/LIVESCHEDULE.csv\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "User: write a paragraph on the scope of the cisco project\n",
            "User: write a paragraph on the scope of the cisco project\n",
            "Chatbot:\n",
            "Retrieving information...\n",
            "The Cisco project aims to create a web-based platform for password rotation, retrieval, and other administrative tasks regarding user accounts. The system will adhere to UPS IT standards and focus on enhancing security and streamlining the password retrieval process. Users will be authenticated based on their assigned roles, and passwords will be rotated every 24 hours, with requests subject to approval. The project will leverage technologies such as .NET MVC 5 and .NET WCF Interface to achieve its objectives.\n",
            "\n",
            "CITATIONS:\n",
            "{'start': 26, 'end': 143, 'text': 'create a web-based platform for password rotation, retrieval, and other administrative tasks regarding user accounts.', 'document_ids': ['doc_0', 'doc_2']}\n",
            "{'start': 170, 'end': 186, 'text': 'UPS IT standards', 'document_ids': ['doc_2']}\n",
            "{'start': 200, 'end': 267, 'text': 'enhancing security and streamlining the password retrieval process.', 'document_ids': ['doc_0']}\n",
            "{'start': 268, 'end': 325, 'text': 'Users will be authenticated based on their assigned roles', 'document_ids': ['doc_2']}\n",
            "{'start': 331, 'end': 407, 'text': 'passwords will be rotated every 24 hours, with requests subject to approval.', 'document_ids': ['doc_2']}\n",
            "{'start': 455, 'end': 488, 'text': '.NET MVC 5 and .NET WCF Interface', 'document_ids': ['doc_0']}\n",
            "\n",
            "\n",
            "DOCUMENTS:\n",
            "Scope: /content/local_data/ScopeDocument.docx\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}